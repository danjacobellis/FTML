{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267e4f8c-85dd-4f36-9ee4-0462952afe5d",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "# Lossy compression to prevent evasion and poisoning\n",
    "\n",
    "[Slides](https://danjacobellis.github.io/FTML/compress.slides.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e072a5-d669-4465-9a53-03cf7f807acd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<script>\n",
    "    document.querySelector('head').innerHTML += '<style>.slides { zoom: 1.75 !important; }</style>';\n",
    "</script>\n",
    "\n",
    "<center> <h1>\n",
    "Lossy compression to prevent evasion and poisoning\n",
    "</h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341ce89f-dd34-4461-ac0b-f616194c5866",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Types of data threats\n",
    "\n",
    "* Evasion\n",
    "  * Exploit knowledge a model that's already been trained\n",
    "  * Example: email spam filter\n",
    "    * Attacker wants to avoid detection while preserving the semantic an email\n",
    "    * Full or partial knowledge of model can be used to find \"magic words\" that cause an email to be classified as not spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cceef4-a58e-4224-ad47-f25c36d2dcfe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Types of data threats\n",
    "\n",
    "* Poisoning\n",
    "  * Attacker contaminates dataset causing major changes to behavior of model\n",
    "  * Example: facial recognition\n",
    "    * Attacker wants to prevent facial recognition from working on one or more subjects\n",
    "    * Attacker uploads altered image to public where dataset is sourced for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d827aed-d51e-4c20-bfae-07edc379ceed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Fast gradient sign method\n",
    "$$adv\\_x = x + \\epsilon*\\text{sign}(\\nabla_xJ(\\theta, x, y))$$\n",
    "![](img/doog.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c299f879-228f-400e-8d04-0d772448e267",
   "metadata": {},
   "source": [
    "## \"Robust\" features\n",
    "* \"Overall, attaining models that are robust and interpretable will require explicitly encoding human priors into the training process\"\n",
    "\n",
    "![](img/robust_doog.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63790ddb-bc5f-4f42-ab2f-fc7e0f4850d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Lossy compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9600e0-09bc-45a5-b2db-2e81b61abc93",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/lossy_lossless.png\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301a2407-41b1-4361-bcaf-8a299b831288",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/lossy_lossless.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89dcee-9137-4c99-809c-fdcbccc827ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Lossy compression to prevent evasion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64073461-5ed8-49af-acaa-623be9b75291",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/JPEG_evasion1.png\" width=600 height=600 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b86a3b-262e-4a2b-9e47-1442f64a36f0",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/JPEG_evasion1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905794bd-6ea8-46b5-865e-5e7f5902b8ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/JPEG_evasion2.png\" width=600 height=600 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df00fd50-0938-476c-b9d3-dff041f88c7e",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/JPEG_evasion2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4e8ef-0d35-4dca-ad6b-e8067badce83",
   "metadata": {},
   "source": [
    "## How small of a perturbation is visible?\n",
    "\n",
    "![](img/dither.png)\n",
    "\n",
    "* Most perturbations are imperceptible if contained in the four least significant bits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d3ffc3-a010-445a-97a8-9cba97dc2315",
   "metadata": {},
   "source": [
    "## Defending against larger perturbations\n",
    "\n",
    "![](img/jpeg_vs_gif.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af3006-5d4a-47f8-ac64-5bbb2c6305e2",
   "metadata": {},
   "source": [
    "https://deepai.org/publication/frequency-tuned-universal-adversarial-attacks#S2.SS3.p1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeea2b5-2109-41ae-a05f-67b10eef0dde",
   "metadata": {},
   "source": [
    "## Impact of lossy compression on training\n",
    "\n",
    "![](img/accuracy_vs_fidelity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549ccd3-b843-4cf4-baa4-f92deb21afb0",
   "metadata": {},
   "source": [
    "## Gradient matching\n",
    "\n",
    "* Proposed in 2020 as a more efficient method to poison large datasets\n",
    "  * Attacker chooses a specific image and chooses a label that they want the image to be classified as\n",
    "  * Using small perturbations to as little as 0.1% of the dataset, the chosen image can be classified as desired by the attacker\n",
    "  * Requires larger perturbations that evasion attacks but are still mostly imperceptible\n",
    "  * Does not require full knowledge of model architecture. Shown to translate to different models\n",
    "    * Example: Poisoning data assuming a resnet20 model still works when a VGG13 model is trained  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d208f40-1e9e-4bc1-bae4-cf958536b859",
   "metadata": {},
   "source": [
    "## Sanitization to prevent poisoning\n",
    "\n",
    "* Test accuracy vs fidelity plot\n",
    "    * JPEG 0%\n",
    "    * JPEG 25%\n",
    "    * JPEG 50%\n",
    "    * JPEG 75%\n",
    "    * JPEG 100%\n",
    "    * Uncompressed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3fc12-45b0-4124-bae8-7789650b5e9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Compressed training to prevent poisoning\n",
    "\n",
    "* An attacker wants the poisoning attack to be imperceptible\n",
    "* Requires the \"poison\" part of the data to be mostly contained in the least significant bits or high frequencies\n",
    "* Instead of training in pixel space, train on quantized transform coefficients\n",
    "  * Details in the least significant bits as well as high frequencies will be discarded\n",
    "  * Details important for classification are kept\n",
    "  * Poisoning may still be possible, but poisoned samples will be much easier to detect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
