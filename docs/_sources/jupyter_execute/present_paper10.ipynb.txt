{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267e4f8c-85dd-4f36-9ee4-0462952afe5d",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "# Understanding Black-box Predictions via Influence functions\n",
    "\n",
    "[Paper](http://proceedings.mlr.press/v70/koh17a/koh17a.pdf)\n",
    "\n",
    "[Slides](https://danjacobellis.github.io/FTML/present_paper10.slides.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e072a5-d669-4465-9a53-03cf7f807acd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<script>\n",
    "    document.querySelector('head').innerHTML += '<style>.slides { zoom: 1.75 !important; }</style>';\n",
    "</script>\n",
    "\n",
    "<center> <h1>\n",
    "Understanding Black-box Predictions via Influence functions\n",
    "</h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e51990-bdc7-4e78-b451-a69302323fcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Cook's distance\n",
    "\n",
    "* Dennis Cook, 1970: *Detection of influential observation in linear regression*\n",
    "* \"Overall summary statistics (e.g. $R^2$, $\\beta$) ... present a distorted and misleading picture\"\n",
    "* **Cook's distance**: deviation of the model when point $i$ is excluded\n",
    "\n",
    "$$D_i = \\frac {1}{ps^2} \\sum_{j=1}^{n}{(\\hat y _j - \\hat y_{j(i)}})^2$$\n",
    "\n",
    "* Identify influential points\n",
    "* Identify regions to sample when collecting subsequent data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154deb5-4ee3-4944-9ba5-7ff3e08e46d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Leverage \n",
    "\n",
    "* Also known as \"Self-influence\"\n",
    "* Degree to which the $i$th measured value influences the $i$th predicted value\n",
    "$$h_{ii} = \\frac {\\partial \\hat y_i}{\\partial y_i}$$\n",
    "* $ 0 \\leq h_{ii} \\leq 1$\n",
    "* Common heuristic: $x_i$ is an outlier if $h_{ii} > \\frac {2p}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00817656-9335-4240-a5e3-41de6bf4b13f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Influence function\n",
    "\n",
    "* Consider a model parameter $\\theta$ and our estimate of the best value after training $\\hat \\theta $\n",
    "* $\\hat \\theta$ is a function of the data distribution $F$\n",
    "* The influence function describes the effect on $\\hat \\theta$ when we make a small perturbation to the data\n",
    "* In particular, we will add at location $z$ in the distribution $F$ an infinitesimal mass $\\epsilon \\delta_z$, resulting in a new distribution $F^\\prime$\n",
    "\n",
    "$$F^\\prime = F(1-\\epsilon) + \\epsilon \\delta_z$$\n",
    "\n",
    "* The influence function is the derivative${}^\\dagger$ of a model parameter $\\hat \\theta$ with respect to the distribution $F$. \n",
    "\n",
    "$$\\mathcal I_{\\hat \\theta}(z) = \\frac{d \\hat \\theta}{d F}^\\dagger =  \\lim_{\\epsilon \\to 0} \\left[ \\frac{ \\hat \\theta(F^\\prime) - \\hat \\theta (F) }{\\epsilon}\\right]$$\n",
    "\n",
    "\n",
    "$\\dagger$ The derivative here is a functional derivative. In particular it is a Gateaux derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a570d-15b4-41a7-af5c-b08b91a11c1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Influence function\n",
    "\n",
    "* We can also define another type of influence function that describes the effect on the loss when we make a small perturbation to the data\n",
    "\n",
    "$$\\mathcal I_{L}(z) = \\frac{d L}{d F} =  \\lim_{\\epsilon \\to 0} \\left[ \\frac{ \\hat L(F^\\prime) - L (F) }{\\epsilon}\\right]$$\n",
    "\n",
    "* A model is trained resulting in parameters $\\hat \\theta$. We test it on a point $z_{test}$. $\\mathcal I_{L}(z,z_{test})$ is the rate at which the loss on $z_{test}$  increases as we make the training point $z$ more prevalent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbfcfb8-79c8-4104-9a17-a479d5e6d79c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Empirical influence function\n",
    "\n",
    "### Expensive approach: Leave one out\n",
    "* Train the model with all data, including some sample $z$, resulting in $\\hat \\theta$\n",
    "* To estimate $\\mathcal I_{\\hat \\theta}(z)$, train the model without the point $z$, resulting in $\\hat \\theta _ {(z)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e049ae2-0192-4cce-ac77-068afd015208",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Empirical influence function\n",
    "\n",
    "### Cheaper approach: Locally approximate shape of loss function\n",
    "* Start with a model trained on all of the data, including $z,$ resulting in $\\hat \\theta$\n",
    "* Use a quadratic approximation of the loss function to estimate the effect of \"upweighting\" a sample $z$.\n",
    "\n",
    "$$\\mathcal I_{\\hat \\theta}(z) \\approx -H_{\\hat \\theta} ^{-1} \\nabla L$$\n",
    "\n",
    "* $\\nabla L$ is the gradient. $H$ is the *Hessian,* the matrix of all partial second derivatives.\n",
    "\n",
    "* $-H_{\\hat \\theta} ^{-1} \\nabla L$ is the direction you would move in while optimizing a function using Newton's method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ca9def-9b93-4a08-a12d-ec7dede13d1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Quadratic approximation\n",
    "\n",
    "### Issue #1: Does the Hessian even exist?\n",
    "\n",
    "* We have to pick a nice loss function so that the Hessian is guaranteed to exist.\n",
    "\n",
    "  * Mean absolute error can be replaced with $\\log \\cosh (s)$. \n",
    "  * Hinge with smooth version $t \\log {\\left(1+\\exp{\\left(\\frac{1-s}{t}\\right)}\\right)}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ccdfdb-5faa-46a4-9609-0829bca3fbea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Quadratic approximation\n",
    "\n",
    "### Issue #2: Computing and storing the Hessian\n",
    "\n",
    "* It is ***very*** expensive to compute the Hessian. Even just storing the Hessian for a large neural network might be impossible.\n",
    "\n",
    "* The \"Pearlmutter trick\" allows us to calculate a matrix-vector product with the hessian $\\mathbf {Hv}$ *exactly* with about as much computation as a single gradient evaluation.\n",
    "\n",
    "* FOSS libraries rolled out in 2018 that allow you plug in your model and efficiently compute the influence using this method.\n",
    "\n",
    "  * Popular one is [Darkon](https://github.com/darkonhub/darkon)\n",
    "  \n",
    "  * [Example notebook]( https://github.com/darkonhub/darkon-examples/blob/master/cifar10-resnet/influence_cifar10_resnet.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d65b6d-34db-438e-8aaf-c6896cd80796",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Applications of influence functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ca29d-8563-4e2d-ae8a-b3586765d907",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Find helpful and harmful training samples\n",
    "\n",
    "* Other methods exist to find relevant training samples.\n",
    "  * For example, we can look at the nearest neighbor(s)\n",
    "  \n",
    "* The influence function can tell us if a training sample either helps or hurts when evaluating a particular test sample.\n",
    "  * If $\\mathcal I_{L}(z,z_{test})$ is positive, then the training point $z$ is harmful\n",
    "  * If $\\mathcal I_{L}(z,z_{test})$ is negative, then the training point $z$ is helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877dd2bb-4df5-4010-a459-2116d19135f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Identify overfitting and compare models\n",
    "\n",
    "* Compare $\\mathcal I_{L}(z,z_{test})$ with the euclidian distance\n",
    "\n",
    "* If the model is overfitting, then all of the most influential training points will be neighbors.\n",
    "\n",
    "* If the model generalizes, then the influential points will be spread out, not just overfit to neighboring points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cedfff5-f66b-44c4-bdb2-50534a8cd2f6",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/influence_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6049dff9-8f4a-4c2c-83ac-c33bf0e236f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Identify overfitting and compare models\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/influence_1.png\" width=450 height=450 class=\"center\">\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}