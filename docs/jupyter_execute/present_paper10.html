
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Understanding Black-box Predictions via Influence functions &#8212; Dan Jacobellis | FTML</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="tex2jax_ignore mathjax_ignore section" id="understanding-black-box-predictions-via-influence-functions">
<h1>Understanding Black-box Predictions via Influence functions<a class="headerlink" href="#understanding-black-box-predictions-via-influence-functions" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="http://proceedings.mlr.press/v70/koh17a/koh17a.pdf">Paper</a></p>
<p><a class="reference external" href="https://danjacobellis.github.io/FTML/present_paper10.slides.html">Slides</a></p>
<script>
    document.querySelector('head').innerHTML += '<style>.slides { zoom: 1.75 !important; }</style>';
</script>
<center> <h1>
Understanding Black-box Predictions via Influence functions
</h1> </center><div class="section" id="cooks-distance">
<h2>Cook’s distance<a class="headerlink" href="#cooks-distance" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Dennis Cook, 1970: <em>Detection of influential observation in linear regression</em></p></li>
<li><p>“Overall summary statistics (e.g. <span class="math notranslate nohighlight">\(R^2\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span>) … present a distorted and misleading picture”</p></li>
<li><p><strong>Cook’s distance</strong>: deviation of the model when point <span class="math notranslate nohighlight">\(i\)</span> is excluded</p></li>
</ul>
<div class="math notranslate nohighlight">
\[D_i = \frac {1}{ps^2} \sum_{j=1}^{n}{(\hat y _j - \hat y_{j(i)}})^2\]</div>
<ul class="simple">
<li><p>Identify influential points</p></li>
<li><p>Identify regions to sample when collecting subsequent data</p></li>
</ul>
</div>
<div class="section" id="leverage">
<h2>Leverage<a class="headerlink" href="#leverage" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Also known as “Self-influence”</p></li>
<li><p>Degree to which the <span class="math notranslate nohighlight">\(i\)</span>th measured value influences the <span class="math notranslate nohighlight">\(i\)</span>th predicted value
$<span class="math notranslate nohighlight">\(h_{ii} = \frac {\partial \hat y_i}{\partial y_i}\)</span>$</p></li>
<li><p><span class="math notranslate nohighlight">\( 0 \leq h_{ii} \leq 1\)</span></p></li>
<li><p>Common heuristic: <span class="math notranslate nohighlight">\(x_i\)</span> is an outlier if <span class="math notranslate nohighlight">\(h_{ii} &gt; \frac {2p}{n}\)</span></p></li>
</ul>
</div>
<div class="section" id="influence-function">
<h2>Influence function<a class="headerlink" href="#influence-function" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Consider a model parameter <span class="math notranslate nohighlight">\(\theta\)</span> and our estimate of the best value after training <span class="math notranslate nohighlight">\(\hat \theta \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\hat \theta\)</span> is a function of the data distribution <span class="math notranslate nohighlight">\(F\)</span></p></li>
<li><p>The influence function describes the effect on <span class="math notranslate nohighlight">\(\hat \theta\)</span> when we make a small perturbation to the data</p></li>
<li><p>In particular, we will add at location <span class="math notranslate nohighlight">\(z\)</span> in the distribution <span class="math notranslate nohighlight">\(F\)</span> an infinitesimal mass <span class="math notranslate nohighlight">\(\epsilon \delta_z\)</span>, resulting in a new distribution <span class="math notranslate nohighlight">\(F^\prime\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[F^\prime = F(1-\epsilon) + \epsilon \delta_z\]</div>
<ul class="simple">
<li><p>The influence function is the derivative<span class="math notranslate nohighlight">\({}^\dagger\)</span> of a model parameter <span class="math notranslate nohighlight">\(\hat \theta\)</span> with respect to the distribution <span class="math notranslate nohighlight">\(F\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathcal I_{\hat \theta}(z) = \frac{d \hat \theta}{d F}^\dagger =  \lim_{\epsilon \to 0} \left[ \frac{ \hat \theta(F^\prime) - \hat \theta (F) }{\epsilon}\right]\]</div>
<p><span class="math notranslate nohighlight">\(\dagger\)</span> The derivative here is a functional derivative. In particular it is a Gateaux derivative.</p>
</div>
<div class="section" id="id1">
<h2>Influence function<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>We can define a second type of influence function that describes the effect on the loss when we make a small perturbation to the data</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathcal I_{L}(z) = \frac{d L}{d F} =  \lim_{\epsilon \to 0} \left[ \frac{ \hat L(F^\prime) - L (F) }{\epsilon}\right]\]</div>
<ul class="simple">
<li><p>A model is trained resulting in parameters <span class="math notranslate nohighlight">\(\hat \theta\)</span>. We test it on a point <span class="math notranslate nohighlight">\(z_{test}\)</span>. <span class="math notranslate nohighlight">\(\mathcal I_{L}(z,z_{test})\)</span> is the rate at which the loss on <span class="math notranslate nohighlight">\(z_{test}\)</span>  increases as we make the training point <span class="math notranslate nohighlight">\(z\)</span> more prevalent</p></li>
</ul>
</div>
<div class="section" id="id2">
<h2>Influence Function<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>We can also define influence functions that describe the effect of ‘moving’ a training point</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathcal I_{pert} = \mathcal I(z_2) -  \mathcal I(z_1)\]</div>
</div>
<div class="section" id="empirical-influence-function">
<h2>Empirical influence function<a class="headerlink" href="#empirical-influence-function" title="Permalink to this headline">¶</a></h2>
<div class="section" id="expensive-approach-leave-one-out">
<h3>Expensive approach: Leave one out<a class="headerlink" href="#expensive-approach-leave-one-out" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Train the model with all data, including some sample <span class="math notranslate nohighlight">\(z\)</span>, resulting in <span class="math notranslate nohighlight">\(\hat \theta\)</span></p></li>
<li><p>To estimate <span class="math notranslate nohighlight">\(\mathcal I_{\hat \theta}(z)\)</span>, train the model without the point <span class="math notranslate nohighlight">\(z\)</span>, resulting in <span class="math notranslate nohighlight">\(\hat \theta _ {(z)}\)</span></p></li>
</ul>
</div>
</div>
<div class="section" id="id3">
<h2>Empirical influence function<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<div class="section" id="cheaper-approach-locally-approximate-shape-of-loss-function">
<h3>Cheaper approach: Locally approximate shape of loss function<a class="headerlink" href="#cheaper-approach-locally-approximate-shape-of-loss-function" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Start with a model trained on all of the data, including <span class="math notranslate nohighlight">\(z,\)</span> resulting in <span class="math notranslate nohighlight">\(\hat \theta\)</span></p></li>
<li><p>Use a quadratic approximation of the loss function to estimate the effect of “upweighting” a sample <span class="math notranslate nohighlight">\(z\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathcal I_{\hat \theta}(z) \approx -H_{\hat \theta} ^{-1} \nabla L\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\nabla L\)</span> is the gradient. <span class="math notranslate nohighlight">\(H\)</span> is the <em>Hessian,</em> the matrix of all partial second derivatives.</p></li>
<li><p><span class="math notranslate nohighlight">\(-H_{\hat \theta} ^{-1} \nabla L\)</span> is the direction you would move in while optimizing a function using Newton’s method.</p></li>
</ul>
</div>
</div>
<div class="section" id="quadratic-approximation">
<h2>Quadratic approximation<a class="headerlink" href="#quadratic-approximation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="issue-1-does-the-hessian-even-exist">
<h3>Issue #1: Does the Hessian even exist?<a class="headerlink" href="#issue-1-does-the-hessian-even-exist" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We have to pick a nice loss function so that the Hessian is guaranteed to exist.</p>
<ul>
<li><p>Mean absolute error can be replaced with <span class="math notranslate nohighlight">\(\log \cosh (s)\)</span>.</p></li>
<li><p>Hinge with smooth version <span class="math notranslate nohighlight">\(t \log {\left(1+\exp{\left(\frac{1-s}{t}\right)}\right)}.\)</span></p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="id4">
<h2>Quadratic approximation<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<div class="section" id="issue-2-computing-and-storing-the-hessian">
<h3>Issue #2: Computing and storing the Hessian<a class="headerlink" href="#issue-2-computing-and-storing-the-hessian" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>It is <em><strong>very</strong></em> expensive to compute the Hessian. Even just storing the Hessian for a large neural network might be impossible.</p></li>
<li><p>The “Pearlmutter trick” allows us to calculate a matrix-vector product with the hessian <span class="math notranslate nohighlight">\(\mathbf {Hv}\)</span> <em>exactly</em> with about as much computation as a single gradient evaluation.</p></li>
<li><p>FOSS libraries rolled out in 2018 that allow you plug in your model and efficiently compute the influence using this method.</p>
<ul>
<li><p>Popular one is <a class="reference external" href="https://github.com/darkonhub/darkon">Darkon</a></p></li>
<li><p><a class="reference external" href="https://github.com/darkonhub/darkon-examples/blob/master/cifar10-resnet/influence_cifar10_resnet.ipynb">Example notebook</a></p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="applications-of-influence-functions">
<h2>Applications of influence functions<a class="headerlink" href="#applications-of-influence-functions" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="find-helpful-and-harmful-training-samples">
<h2>Find helpful and harmful training samples<a class="headerlink" href="#find-helpful-and-harmful-training-samples" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Other methods exist to find relevant training samples.</p>
<ul>
<li><p>For example, we can look at the nearest neighbor(s)</p></li>
</ul>
</li>
<li><p>The influence function can tell us if a training sample either helps or hurts when evaluating a particular test sample.</p>
<ul>
<li><p>If <span class="math notranslate nohighlight">\(\mathcal I_{L}(z,z_{test})\)</span> is positive, then the training point <span class="math notranslate nohighlight">\(z\)</span> is harmful</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\mathcal I_{L}(z,z_{test})\)</span> is negative, then the training point <span class="math notranslate nohighlight">\(z\)</span> is helpful</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="jupyter_execute/img/harmful_1.png" /></p>
</div>
<div class="section" id="identify-overfitting-and-compare-models">
<h2>Identify overfitting and compare models<a class="headerlink" href="#identify-overfitting-and-compare-models" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Compare <span class="math notranslate nohighlight">\(\mathcal I_{L}(z,z_{test})\)</span> with the euclidian distance</p></li>
<li><p>If the model is overfitting, then all of the most influential training points will be neighbors.</p></li>
<li><p>If the model generalizes, then the influential points will be spread out, not just overfit to neighboring points.</p></li>
</ul>
<p><img alt="" src="jupyter_execute/img/influence_1.png" /></p>
</div>
<div class="section" id="debugging-mismatch-in-distributions">
<h2>Debugging mismatch in distributions<a class="headerlink" href="#debugging-mismatch-in-distributions" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Consider a point the sample space <span class="math notranslate nohighlight">\(z\)</span> and the surrounding region</p></li>
<li><p>The training data may be very dense or very sparse in this region</p></li>
<li><p>When deployed, the density may be quite different</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal I_{pert}(z)\)</span> tells us exactly how we should update the model to account for this</p></li>
</ul>
</div>
<div class="section" id="debugging-mislabeled-examples-and-identifying-places-to-collect-new-data">
<h2>Debugging mislabeled examples and identifying places to collect new data<a class="headerlink" href="#debugging-mislabeled-examples-and-identifying-places-to-collect-new-data" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Checking the top 5% highest magnitude influence points gives roughly the same performance as checking 60% of the training data exhaustively</p></li>
</ul>
<p><img alt="" src="jupyter_execute/img/mislabeled_1.png" /></p>
<ul class="simple">
<li><p>Find an example that is difficult or ambiguous to start with</p></li>
<li><p>At this point, find the gradient of the loss with respect to the data <span class="math notranslate nohighlight">\(\nabla _x L\)</span></p></li>
<li><p>Training on the point <span class="math notranslate nohighlight">\(x + \mu \nabla _x L\)</span> will massively increase the error rate on any data near <span class="math notranslate nohighlight">\(x\)</span></p></li>
<li><p>The magnitude of <span class="math notranslate nohighlight">\(\mathcal I_{pert}(z)\)</span> tells us how vulnerable a model is to an attack near <span class="math notranslate nohighlight">\(z\)</span></p></li>
</ul>
<p><img alt="" src="jupyter_execute/img/adversarial_1.png" /></p>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">FTML</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../critique_paper2.html">Critique of Paper 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../present_paper10.html">Understanding Black-box Predictions via Influence functions</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.3.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/jupyter_execute/present_paper10.ipynb.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>