
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Understanding Black-box Predictions via Influence functions &#8212; Dan Jacobellis | FTML</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="understanding-black-box-predictions-via-influence-functions">
<h1>Understanding Black-box Predictions via Influence functions<a class="headerlink" href="#understanding-black-box-predictions-via-influence-functions" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="http://proceedings.mlr.press/v70/koh17a/koh17a.pdf">Paper</a></p>
<p><a class="reference external" href="https://danjacobellis.github.io/FTML/present_paper10.slides.html">Slides</a></p>
<script>
    document.querySelector('head').innerHTML += '<style>.slides { zoom: 1.75 !important; }</style>';
</script>
<center> <h1>
Understanding Black-box Predictions via Influence functions
</h1> </center><section id="background-and-caveats">
<h2>Background and caveats<a class="headerlink" href="#background-and-caveats" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Our goal is to gain insights about how particular training points influence a model</p></li>
<li><p>Loose definition of “black box” for this work</p>
<ul>
<li><p>We <strong>do</strong> need to know the relationship between the loss and the model parameters</p></li>
<li><p>However, we don’t need to know anything about the model architecture or how it works. Massive DNNs are fine.</p></li>
</ul>
</li>
</ul>
<p>The idea of examining the influence of particular training samples dates back to at least the 1970s.</p>
<p>For example, something that is used commonly in linear regression is Cook’s distance.</p>
<p>Cook’s distance is pretty simple. We want to know how an individual sample affects the prediction</p>
<p>To find the influence of the ith point, we train a model without that point, and observe the prediction. Here that would be this y with parentheses i.</p>
<p>We sum up the differences between the predictions with that point and without that point and then normalize by the number of parameters p and the mean squared error s squared.</p>
<p>That’s cook’s distance. It’s quite useful. We can use it to find influential points for many reasons. We can go see if maybe that point was mislabeled. We can try to understand why that point was influential.</p>
<p>We can also use it to try to identify where we should sample the next time we go collect data.</p>
</section>
<section id="cooks-distance">
<h2>Cook’s distance<a class="headerlink" href="#cooks-distance" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Dennis Cook, 1970: <em>Detection of influential observation in linear regression</em></p></li>
<li><p>“Overall summary statistics (e.g. <span class="math notranslate nohighlight">\(R^2\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span>) … present a distorted and misleading picture”</p></li>
<li><p><strong>Cook’s distance</strong>: deviation of the model when point <span class="math notranslate nohighlight">\(i\)</span> is excluded</p></li>
</ul>
<div class="math notranslate nohighlight">
\[D_i = \frac {1}{ps^2} \sum_{j=1}^{n}{(\hat y _j - \hat y_{j(i)}})^2\]</div>
<ul class="simple">
<li><p>Identify influential points</p></li>
<li><p>Identify regions to sample when collecting subsequent data</p></li>
</ul>
<p>A related quantity is the leverage. You can think of this as the degree to which the ith measured value influences the ith predicted value.</p>
<p>This is often used to detect outliers. A common heuristic is that if the leverage exceeds some multiple of p over n, the ratio of parameters to samples, then a point is an outlier.</p>
<p>Next, I’ll define some different types of influence functions but just keep in mind that these functions are trying to do something similar to cook’s distance or leverage, where we’re looking at how particular point influence the model.</p>
</section>
<section id="leverage">
<h2>Leverage<a class="headerlink" href="#leverage" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Also known as “Self-influence”</p></li>
<li><p>Degree to which the <span class="math notranslate nohighlight">\(i\)</span>th measured value influences the <span class="math notranslate nohighlight">\(i\)</span>th predicted value
$<span class="math notranslate nohighlight">\(h_{ii} = \frac {\partial \hat y_i}{\partial y_i}\)</span>$</p></li>
<li><p><span class="math notranslate nohighlight">\( 0 \leq h_{ii} \leq 1\)</span></p></li>
<li><p>Common heuristic: <span class="math notranslate nohighlight">\(x_i\)</span> is an outlier if <span class="math notranslate nohighlight">\(h_{ii} &gt; \frac {2p}{n}\)</span></p></li>
</ul>
<p>Let’s start with the first type of influence function. Suppose you train a model. You minimize the empirical risk, resulting in some estimate of the best model parameters theta hat.</p>
<p>This theta hat is a function of the data distribution F. If we had a different set of data that we trained on that came from a different distribution, we would have gotten different estimates for our parameters.</p>
<p>The influence function is going to examine how a small perturbation in F affects theta hat.</p>
<p>Lets define F prime to be some new distribution that is almost the same as the original, except we’ve added some mass to a location in the sample space z.</p>
<p>The first type of influence function which we denote I theta, is like a derivative. It’s the difference in model parameters that we get when changing the distribution in the limit that that change is very small.</p>
<p>If we’re being very rigorous this is not a normal derivative but a functional one. But I think it’s helpful to think about it the same way you think of any other derivative, as a rate of change.</p>
<p>So that’s the first type of influence function, the rate of change of the model parameters when we add mass to some point in the distribution z.</p>
</section>
<section id="influence-function">
<h2>Influence function<a class="headerlink" href="#influence-function" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Consider a model parameter <span class="math notranslate nohighlight">\(\theta\)</span> and our estimate of the best value after training <span class="math notranslate nohighlight">\(\hat \theta \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\hat \theta\)</span> is a function of the data distribution <span class="math notranslate nohighlight">\(F\)</span></p></li>
<li><p>The influence function describes the effect on <span class="math notranslate nohighlight">\(\hat \theta\)</span> when we make a small perturbation to the data</p></li>
<li><p>In particular, we will add at location <span class="math notranslate nohighlight">\(z\)</span> in the distribution <span class="math notranslate nohighlight">\(F\)</span> an infinitesimal mass <span class="math notranslate nohighlight">\(\epsilon \delta_z\)</span>, resulting in a new distribution <span class="math notranslate nohighlight">\(F^\prime\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[F^\prime = F(1-\epsilon) + \epsilon \delta_z\]</div>
<ul class="simple">
<li><p>The influence function is the derivative<span class="math notranslate nohighlight">\({}^\dagger\)</span> of a model parameter <span class="math notranslate nohighlight">\(\hat \theta\)</span> with respect to the distribution <span class="math notranslate nohighlight">\(F\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathcal I_{\hat \theta}(z) = \frac{d \hat \theta}{d F}^\dagger =  \lim_{\epsilon \to 0} \left[ \frac{ \hat \theta(F^\prime) - \hat \theta (F) }{\epsilon}\right]\]</div>
<p><span class="math notranslate nohighlight">\(\dagger\)</span> The derivative here is a functional derivative. In particular it is a Gateaux derivative.</p>
<p>The second type of influence function IL is similar, except instead of looking at the model parameters, we’re looking at the loss function.</p>
<p>So, this is the rate of change in the loss function as we add mass to some location z.</p>
</section>
<section id="id1">
<h2>Influence function<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>We can define a second type of influence function that describes the effect on the loss when we make a small perturbation to the data</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathcal I_{L}(z) = \frac{d L}{d F} =  \lim_{\epsilon \to 0} \left[ \frac{ \hat L(F^\prime) - L (F) }{\epsilon}\right]\]</div>
<ul class="simple">
<li><p>A model is trained resulting in parameters <span class="math notranslate nohighlight">\(\hat \theta\)</span>. We test it on a point <span class="math notranslate nohighlight">\(z_{test}\)</span>. <span class="math notranslate nohighlight">\(\mathcal I_{L}(z,z_{test})\)</span> is the rate at which the loss on <span class="math notranslate nohighlight">\(z_{test}\)</span>  increases as we make the training point <span class="math notranslate nohighlight">\(z\)</span> more prevalent</p></li>
</ul>
<p>Finally, we have this third type of influence function I pert</p>
<p>Instead of just adding a mass to some point in the distribution, we’re also going to remove mass from some other point.</p>
<p>So this is basically just a difference between to influence functions of the normal type.</p>
</section>
<section id="id2">
<h2>Influence Function<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>We can also define influence functions that describe the effect of ‘moving’ a training point</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathcal I_{pert} = \mathcal I(z_2) -  \mathcal I(z_1)\]</div>
<p>So how do we actually calculate these influence functions?</p>
<p>Well the simplest approach is to just do something like we did for cook’s distance and just train the model with all points except one point located at z. That will give us the influence function at z.</p>
<p>Clearly, this can get expensive quickly, so we’d like to have a more efficient method.</p>
<p>We could have tried to calculate the gradient of the model parameters or the loss function with respect to the data for example.</p>
<p>However, the authors go one step further. Why stop at the first derivative when we can use the second derivative? So that’s what they do.</p>
</section>
<section id="empirical-influence-function">
<h2>Empirical influence function<a class="headerlink" href="#empirical-influence-function" title="Permalink to this headline">¶</a></h2>
<section id="expensive-approach-leave-one-out">
<h3>Expensive approach: Leave one out<a class="headerlink" href="#expensive-approach-leave-one-out" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Train the model with all data, including some sample <span class="math notranslate nohighlight">\(z\)</span>, resulting in <span class="math notranslate nohighlight">\(\hat \theta\)</span></p></li>
<li><p>To estimate <span class="math notranslate nohighlight">\(\mathcal I_{\hat \theta}(z)\)</span>, train the model without the point <span class="math notranslate nohighlight">\(z\)</span>, resulting in <span class="math notranslate nohighlight">\(\hat \theta _ {(z)}\)</span></p></li>
</ul>
<p>We train the model with all of the data, resulting in the estimate of the optimal parameter values theta hat.</p>
<p>Starting from there, we locally approximate the loss function as a quadratic.</p>
<p>When we do that we get this result for the influence function.</p>
<p>We have the gradient of the loss function multiplied by the inverse of the hessian.</p>
<p>The hessian is just the matrix of second derivatives.</p>
</section>
</section>
<section id="id3">
<h2>Empirical influence function<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<section id="cheaper-approach-locally-approximate-shape-of-loss-function">
<h3>Cheaper approach: Locally approximate shape of loss function<a class="headerlink" href="#cheaper-approach-locally-approximate-shape-of-loss-function" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Start with a model trained on all of the data, including <span class="math notranslate nohighlight">\(z,\)</span> resulting in <span class="math notranslate nohighlight">\(\hat \theta\)</span></p></li>
<li><p>Use a quadratic approximation of the loss function to estimate the effect of “upweighting” a sample <span class="math notranslate nohighlight">\(z\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathcal I_{\hat \theta}(z) \approx -H_{\hat \theta} ^{-1} \nabla L\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\nabla L\)</span> is the gradient. <span class="math notranslate nohighlight">\(H\)</span> is the <em>Hessian,</em> the matrix of all partial second derivatives.</p></li>
<li><p><span class="math notranslate nohighlight">\(-H_{\hat \theta} ^{-1} \nabla L\)</span> is the direction you would move in while optimizing a function using Newton’s method.</p></li>
</ul>
</section>
</section>
<section id="quadratic-approximation">
<h2>Quadratic approximation<a class="headerlink" href="#quadratic-approximation" title="Permalink to this headline">¶</a></h2>
<section id="issue-1-does-the-hessian-even-exist">
<h3>Issue #1: Does the Hessian even exist?<a class="headerlink" href="#issue-1-does-the-hessian-even-exist" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We have to pick a nice loss function so that the Hessian is guaranteed to exist.</p>
<ul>
<li><p>Mean absolute error can be replaced with <span class="math notranslate nohighlight">\(\log \cosh (s)\)</span>.</p></li>
<li><p>Hinge with smooth version <span class="math notranslate nohighlight">\(t \log {\left(1+\exp{\left(\frac{1-s}{t}\right)}\right)}.\)</span></p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="id4">
<h2>Quadratic approximation<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<section id="issue-2-computing-and-storing-the-hessian">
<h3>Issue #2: Computing and storing the Hessian<a class="headerlink" href="#issue-2-computing-and-storing-the-hessian" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>It is <em><strong>very</strong></em> expensive to compute the Hessian. Even just storing the Hessian for a large neural network might be impossible.</p></li>
<li><p>The “Pearlmutter trick” allows us to calculate a matrix-vector product with the hessian <span class="math notranslate nohighlight">\(\mathbf {Hv}\)</span> <em>exactly</em> with about as much computation as a single gradient evaluation.</p></li>
<li><p>There are variations of how to do this. Some are exact some are not with tradeoffs in complexity.</p></li>
<li><p>FOSS libraries rolled out in 2018 that allow you plug in your model and efficiently compute the influence using this method.</p>
<ul>
<li><p>Popular one is <a class="reference external" href="https://github.com/darkonhub/darkon">Darkon</a></p></li>
<li><p><a class="reference external" href="https://github.com/darkonhub/darkon-examples/blob/master/cifar10-resnet/influence_cifar10_resnet.ipynb">Example notebook</a></p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="experiments-and-applications">
<h2>Experiments and Applications<a class="headerlink" href="#experiments-and-applications" title="Permalink to this headline">¶</a></h2>
</section>
<section id="how-well-does-the-quadratic-approximation-work">
<h2>How well does the quadratic approximation work?<a class="headerlink" href="#how-well-does-the-quadratic-approximation-work" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="jupyter_execute/img/performance_1.png" /></p>
</section>
<section id="do-differentiable-versions-of-the-loss-function-work">
<h2>Do differentiable versions of the loss function work?<a class="headerlink" href="#do-differentiable-versions-of-the-loss-function-work" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="jupyter_execute/img/hinge_1.png" /></p>
</section>
<section id="find-helpful-and-harmful-training-samples">
<h2>Find helpful and harmful training samples<a class="headerlink" href="#find-helpful-and-harmful-training-samples" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Other methods exist to find relevant training samples.</p>
<ul>
<li><p>For example, we can look at the nearest neighbor(s)</p></li>
</ul>
</li>
<li><p>The influence function can tell us if a training sample either helps or hurts when evaluating a particular test sample.</p>
<ul>
<li><p>If <span class="math notranslate nohighlight">\(\mathcal I_{L}(z,z_{test})\)</span> is positive, then the training point <span class="math notranslate nohighlight">\(z\)</span> is harmful</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\mathcal I_{L}(z,z_{test})\)</span> is negative, then the training point <span class="math notranslate nohighlight">\(z\)</span> is helpful</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="jupyter_execute/img/harmful_1.png" /></p>
</section>
<section id="identify-overfitting-and-compare-models">
<h2>Identify overfitting and compare models<a class="headerlink" href="#identify-overfitting-and-compare-models" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Compare <span class="math notranslate nohighlight">\(\mathcal I_{L}(z,z_{test})\)</span> with the euclidian distance</p></li>
<li><p>If the model is overfitting, then all of the most influential training points will be neighbors.</p></li>
<li><p>If the model generalizes, then the influential points will be spread out, not just overfit to neighboring points.</p></li>
</ul>
<p><img alt="" src="jupyter_execute/img/influence_1.png" /></p>
</section>
<section id="debugging-mismatch-in-distributions">
<h2>Debugging mismatch in distributions<a class="headerlink" href="#debugging-mismatch-in-distributions" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Consider a point the sample space <span class="math notranslate nohighlight">\(z\)</span> and the surrounding region</p></li>
<li><p>The training data may be very dense or very sparse in this region</p></li>
<li><p>When deployed, the density may be quite different</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal I_{\hat \theta, pert}(z)\)</span> tells us exactly how we should update the model to account for this</p></li>
</ul>
</section>
<section id="debugging-mislabeled-examples-and-identifying-places-to-collect-new-data">
<h2>Debugging mislabeled examples and identifying places to collect new data<a class="headerlink" href="#debugging-mislabeled-examples-and-identifying-places-to-collect-new-data" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Checking the top 5% highest magnitude influence points gives roughly the same performance as checking 60% of the training data exhaustively</p></li>
</ul>
<p><img alt="" src="jupyter_execute/img/mislabeled_1.png" /></p>
</section>
<section id="constructing-and-identifying-adversarial-examples">
<h2>Constructing and identifying adversarial examples<a class="headerlink" href="#constructing-and-identifying-adversarial-examples" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Find an example that is difficult or ambiguous to start with</p></li>
<li><p>At this point, find the gradient of the loss with respect to the data <span class="math notranslate nohighlight">\(\nabla _x L\)</span></p></li>
<li><p>Training on the point <span class="math notranslate nohighlight">\(x + \mu \nabla _x L\)</span> will massively increase the error rate on any data near <span class="math notranslate nohighlight">\(x\)</span></p></li>
<li><p>The magnitude of <span class="math notranslate nohighlight">\(\mathcal I_{pert}(z)\)</span> tells us how vulnerable a model is to an attack near <span class="math notranslate nohighlight">\(z\)</span></p></li>
</ul>
<p><img alt="" src="jupyter_execute/img/adversarial_1.png" /></p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">FTML</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../critique_paper2.html">Critique of Paper 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../present_paper10.html">Understanding Black-box Predictions via Influence functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../critique_50y_fairness.html">Critique of “50 Years of Test (Un)fairness”</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.5.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/jupyter_execute/present_paper10.ipynb.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>