
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Lossy compression to prevent evasion and poisoning &#8212; Dan Jacobellis | FTML</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/theme.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="A review of uncertainty quantification in deep learning" href="uncertainty_quantification.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="lossy-compression-to-prevent-evasion-and-poisoning">
<h1>Lossy compression to prevent evasion and poisoning<a class="headerlink" href="#lossy-compression-to-prevent-evasion-and-poisoning" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://danjacobellis.github.io/FTML/compress.slides.html">Slides</a></p>
<section id="types-of-data-threats">
<h2>Types of data threats<a class="headerlink" href="#types-of-data-threats" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Evasion</p>
<ul>
<li><p>Exploit knowledge a model that’s already been trained</p></li>
<li><p>Example: email spam filter</p>
<ul>
<li><p>Attacker wants to avoid detection while preserving the semantic an email</p></li>
<li><p>Full or partial knowledge of model can be used to find “magic words” that cause an email to be classified as not spam</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="id1">
<h2>Types of data threats<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Poisoning</p>
<ul>
<li><p>Attacker contaminates dataset causing major changes to behavior of model</p></li>
<li><p>Example: facial recognition</p>
<ul>
<li><p>Attacker wants to prevent facial recognition from working on one or more subjects</p></li>
<li><p>Attacker uploads altered image to public where dataset is sourced for training</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="fast-gradient-sign-method">
<h2>Fast gradient sign method<a class="headerlink" href="#fast-gradient-sign-method" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[adv\_x = x + \epsilon*\text{sign}(\nabla_xJ(\theta, x, y))\]</div>
<p><img alt="" src="_images/doog.png" /></p>
</section>
<section id="robust-features">
<h2>“Robust” features<a class="headerlink" href="#robust-features" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>“Overall, attaining models that are robust and interpretable will require explicitly encoding human priors into the training process”</p></li>
</ul>
<p><img alt="" src="_images/robust_doog.png" /></p>
</section>
<section id="lossy-compression">
<h2>Lossy compression<a class="headerlink" href="#lossy-compression" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="_images/lossy_lossless.png" /></p>
</section>
<section id="lossy-compression-to-prevent-evasion">
<h2>Lossy compression to prevent evasion<a class="headerlink" href="#lossy-compression-to-prevent-evasion" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="_images/JPEG_evasion1.png" /></p>
<p><img alt="" src="_images/JPEG_evasion2.png" /></p>
</section>
<section id="how-small-of-a-perturbation-is-visible">
<h2>How small of a perturbation is visible?<a class="headerlink" href="#how-small-of-a-perturbation-is-visible" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://deepai.org/publication/frequency-tuned-universal-adversarial-attacks#S2.SS3.p1">https://deepai.org/publication/frequency-tuned-universal-adversarial-attacks#S2.SS3.p1</a></p>
</section>
<section id="gradient-matching">
<h2>Gradient matching<a class="headerlink" href="#gradient-matching" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Proposed in 2020 as a more efficient method to poison large datasets</p>
<ul>
<li><p>Attacker chooses a specific image and chooses a label that they want the image to be classified as</p></li>
<li><p>Using small perturbations to as little as 0.1% of the dataset, the chosen image can be classified as desired by the attacker</p></li>
<li><p>Requires larger perturbations that evasion attacks but are still mostly imperceptible</p></li>
<li><p>Does not require full knowledge of model architecture. Shown to translate to different models</p>
<ul>
<li><p>Example: Poisoning data assuming a resnet20 model still works when a VGG13 model is trained</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="impact-of-lossy-compression-on-training">
<h2>Impact of lossy compression on training<a class="headerlink" href="#impact-of-lossy-compression-on-training" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Accuracy on attacked image vs fidelity</p></li>
</ul>
</section>
<section id="sanitization-to-prevent-poisoning">
<h2>Sanitization to prevent poisoning<a class="headerlink" href="#sanitization-to-prevent-poisoning" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Test accuracy vs fidelity plot</p>
<ul>
<li><p>JPEG 0%</p></li>
<li><p>JPEG 25%</p></li>
<li><p>JPEG 50%</p></li>
<li><p>JPEG 75%</p></li>
<li><p>JPEG 100%</p></li>
<li><p>Uncompressed</p></li>
</ul>
</li>
</ul>
</section>
<section id="compressed-training-to-prevent-poisoning">
<h2>Compressed training to prevent poisoning<a class="headerlink" href="#compressed-training-to-prevent-poisoning" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>An attacker wants the poisoning attack to be imperceptible</p></li>
<li><p>Requires the “poison” part of the data to be mostly contained in the least significant bits or high frequencies</p></li>
<li><p>Instead of training in pixel space, train on quantized transform coefficients</p>
<ul>
<li><p>Details in the least significant bits as well as high frequencies will be discarded</p></li>
<li><p>Details important for classification are kept</p></li>
<li><p>Poisoning may still be possible, but poisoned samples will be much easier to detect</p></li>
</ul>
</li>
</ul>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">FTML</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="critique_paper2.html">Critique of Paper 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="present_paper10.html">Understanding Black-box Predictions via Influence functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="critique_50y_fairness.html">Critique of “50 Years of Test (Un)fairness”</a></li>
<li class="toctree-l1"><a class="reference internal" href="survey_of_datasets.html">A survey on datasets for fairness-aware machine learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="proposal.html">Proposal: Training on Lossy Encoded Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="failing_loudly.html">Critique of Critique of “Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift”</a></li>
<li class="toctree-l1"><a class="reference internal" href="uncertainty_quantification.html">A review of uncertainty quantification in deep learning</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Lossy compression to prevent evasion and poisoning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#types-of-data-threats">Types of data threats</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id1">Types of data threats</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fast-gradient-sign-method">Fast gradient sign method</a></li>
<li class="toctree-l2"><a class="reference internal" href="#robust-features">“Robust” features</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lossy-compression">Lossy compression</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lossy-compression-to-prevent-evasion">Lossy compression to prevent evasion</a></li>
<li class="toctree-l2"><a class="reference internal" href="#how-small-of-a-perturbation-is-visible">How small of a perturbation is visible?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gradient-matching">Gradient matching</a></li>
<li class="toctree-l2"><a class="reference internal" href="#impact-of-lossy-compression-on-training">Impact of lossy compression on training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sanitization-to-prevent-poisoning">Sanitization to prevent poisoning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#compressed-training-to-prevent-poisoning">Compressed training to prevent poisoning</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="uncertainty_quantification.html" title="previous chapter">A review of uncertainty quantification in deep learning</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.5.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/compress.ipynb.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>