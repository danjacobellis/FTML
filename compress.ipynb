{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267e4f8c-85dd-4f36-9ee4-0462952afe5d",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "# Lossy compression to prevent evasion and poisoning\n",
    "\n",
    "[Slides](https://danjacobellis.github.io/FTML/compress.slides.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e072a5-d669-4465-9a53-03cf7f807acd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<script>\n",
    "    document.querySelector('head').innerHTML += '<style>.slides { zoom: 1.75 !important; }</style>';\n",
    "</script>\n",
    "\n",
    "<center> <h1>\n",
    "Lossy compression to prevent evasion and poisoning\n",
    "</h1> </center>\n",
    "\n",
    "<center> <h3>\n",
    "Dan Jacobellis, Matthew Qin\n",
    "</h3> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341ce89f-dd34-4461-ac0b-f616194c5866",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Evasion\n",
    "* Exploit knowledge a model that's already been trained\n",
    "* Example: email spam filter\n",
    "  * Attacker wants to avoid detection while preserving the semantic content an email\n",
    "  * Full or partial knowledge of model can be used to find \"magic words\" that cause an email to be classified as not spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cceef4-a58e-4224-ad47-f25c36d2dcfe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Poisoning\n",
    "* Attacker contaminates dataset, usually with the goal of introducing a backdoor\n",
    "* Example: facial recognition\n",
    "  * Attacker wants to prevent facial recognition from working on one or more subjects\n",
    "  * Attacker uploads altered image to public where dataset is sourced for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d827aed-d51e-4c20-bfae-07edc379ceed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Fast gradient sign method\n",
    "$$x_{\\text{adv}} = x + \\epsilon*\\text{sign}(\\nabla_xJ(\\theta, x, y))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b874f1-c536-4aaa-9c56-5deb102773ee",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/doog.png\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6f06d-b6fa-4d01-a7d0-e87db0903084",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/doog.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c299f879-228f-400e-8d04-0d772448e267",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## \"Robust\" features\n",
    "* \"Overall, attaining models that are robust and interpretable will require explicitly encoding human priors into the training process\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6f9c85-7e56-4c48-9ed0-98a3196dac8f",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/robust_doog.png\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c02d8a4-dab2-4179-8c2c-15c82e597f88",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/robust_doog.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63790ddb-bc5f-4f42-ab2f-fc7e0f4850d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Lossy compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9600e0-09bc-45a5-b2db-2e81b61abc93",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/lossy_lossless.png\" width=800 height=800 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301a2407-41b1-4361-bcaf-8a299b831288",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/lossy_lossless.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89dcee-9137-4c99-809c-fdcbccc827ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Lossy compression to prevent evasion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64073461-5ed8-49af-acaa-623be9b75291",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/JPEG_evasion1.png\" width=800 height=800 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b86a3b-262e-4a2b-9e47-1442f64a36f0",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/JPEG_evasion1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45012f1-6308-49d4-b904-c5992879abc6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Lossy compression to prevent evasion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905794bd-6ea8-46b5-865e-5e7f5902b8ff",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/JPEG_evasion2.png\" width=800 height=800 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df00fd50-0938-476c-b9d3-dff041f88c7e",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/JPEG_evasion2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4e8ef-0d35-4dca-ad6b-e8067badce83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Visual perception of quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63293bc8-2da8-4310-be2d-8d6208bb7617",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/dither.png\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345fdb42-8d72-4b3d-90b5-670aa437d546",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/dither.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7378f7c-ed09-487e-b6fe-2d742f71490c",
   "metadata": {},
   "source": [
    "* Most perturbations are imperceptible if contained in the four least significant bits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d3ffc3-a010-445a-97a8-9cba97dc2315",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Defending against larger perturbations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53684165-4d09-44ee-8b9d-4bd9e5e18d02",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/jpeg_vs_gif.png\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a20421-91ee-4bfd-ba55-1826c8eed556",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/jpeg_vs_gif.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af3006-5d4a-47f8-ac64-5bbb2c6305e2",
   "metadata": {
    "tags": [
     "remove-nb-cell",
     "remove-cell"
    ]
   },
   "source": [
    "https://deepai.org/publication/frequency-tuned-universal-adversarial-attacks#S2.SS3.p1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeea2b5-2109-41ae-a05f-67b10eef0dde",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## How lossy to compress?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f01cc7-a62a-4366-a5c2-696bdf6f1c8f",
   "metadata": {},
   "source": [
    "$$ \\text{Performance on perturbed test sample} = \\begin{cases} \\text{label confidence} & \\text{label is correct} \\\\ -\\text{label confidence} & \\text{label is incorrect} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd536fc-b0fe-46d8-a2b1-c88391b1ade6",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/accuracy_vs_fidelity.png\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3ce377-c2fe-4862-90f6-ea7b8f57c313",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/accuracy_vs_fidelity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549ccd3-b843-4cf4-baa4-f92deb21afb0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Gradient matching\n",
    "\n",
    "* Proposed in 2020 as a more efficient method to poison large datasets\n",
    "* Attacker chooses a specific image and chooses a label that they want the image to be classified as\n",
    "* Using small perturbations to as little as 0.1% of the dataset, the chosen image can be classified as desired by the attacker\n",
    "* Requires larger perturbations that evasion attacks but are still mostly imperceptible\n",
    "* Does not require full knowledge of model architecture. Shown to translate to different models\n",
    "* Example: Poisoning data by assuming a resnet20 model still works when a VGG13 model is trained  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d208f40-1e9e-4bc1-bae4-cf958536b859",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Sanitization to prevent poisoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15f53b-4d75-42db-80a4-385add7754eb",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/CIFAR_compare.png\" width=650 height=650 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e676d2-910c-4a86-85f9-81933e121ce3",
   "metadata": {},
   "source": [
    "![](img/CIFAR_compare.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3fc12-45b0-4124-bae8-7789650b5e9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Compressed training to prevent poisoning\n",
    "\n",
    "* An attacker wants the poisoning attack to be imperceptible\n",
    "* Requires the \"poison\" part of the data to be mostly contained in the least significant bits or high frequencies\n",
    "* Instead of training in pixel space, train on quantized transform coefficients\n",
    "  * Discards details in the least significant bits as well as high frequencies\n",
    "  * Details important for classification are kept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd740b42-1aaa-40a3-94f8-7e3eb9cc9c92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Audio classification: baseline\n",
    "* Dataset: Speech commands\n",
    "  * One second speech segments of 8 possible words\n",
    "  * 'stop,' 'down,' 'no,' 'right,' 'go,' 'up,' 'yes,' 'left'\n",
    "* Baseline model:\n",
    "  * Input size: $128 \\times 128$ time-frequency distribution represented at full precision\n",
    "  * 119.52 MiB Feature size\n",
    "  * 2.26 GFLOPs per pass\n",
    "  * Achieves test accuracy of about 84% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d959d167-4550-4bb7-8256-a9668ac4346e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Audio classification: VQ + BNN\n",
    "* Encode 2x2 time-frequency blocks via vector quantization\n",
    "  * Use mini-batch k-means to learn codebook of 16 vectors (4 bits)\n",
    "  * Compression ratio of 16:1 (before any entropy coding)\n",
    "* Input size: $64 \\times 64 \\times 4$ binary codes\n",
    "\n",
    "| <audio controls=\"controls\"><source src=\"./_static/left01.wav\" type=\"audio/wav\"></audio>    | <audio controls=\"controls\"><source src=\"./_static/right01.wav\" type=\"audio/wav\"></audio>    | <audio controls=\"controls\"><source src=\"./_static/yes01.wav\" type=\"audio/wav\"></audio>    | <audio controls=\"controls\"><source src=\"./_static/no01.wav\" type=\"audio/wav\"></audio>    |\n",
    "|--------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|\n",
    "| <audio controls=\"controls\"><source src=\"./_static/left01_vq.wav\" type=\"audio/wav\"></audio> | <audio controls=\"controls\"><source src=\"./_static/right01_vq.wav\" type=\"audio/wav\"></audio> | <audio controls=\"controls\"><source src=\"./_static/yes01_vq.wav\" type=\"audio/wav\"></audio> | <audio controls=\"controls\"><source src=\"./_static/no01_vq.wav\" type=\"audio/wav\"></audio> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc3bbaf-98a0-4b85-83b3-deda91f955fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Audio classification: VQ + BNN\n",
    "* Input size: $64 \\times 64 \\times 4$ binary codes\n",
    "* 3.74 MiB feature size \n",
    "* Multiply-accumulate instead of FP\n",
    "  * 4-way MAC unit uses about 55% of the area of a FP16 FPU\n",
    "  * $4-8 \\times$ more power efficient compared to bfloat16\n",
    "  * $>20 \\times$ more power efficient compared to FP32\n",
    "* Achieves test accuracy of about 79% (down from baseline of 84%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
